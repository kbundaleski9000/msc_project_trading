{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ae6181a",
   "metadata": {},
   "source": [
    "## 1-period, 2-player MARL setting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471b4250",
   "metadata": {},
   "source": [
    "Changes made\n",
    "* Env:\n",
    "  * Changed obs to be `I: [v, p0, sigma_v, sigma_u]` and `MM: [y, p0, sigma_v, sigma_u]`\n",
    "    * made order consistent (`v, y` at start and distribution params at the end), also changed any code that does e.g. `obs[0]=v` to account for that\n",
    "    * removed `**2` from the stdevs\n",
    "  * action scaling as an explicit variable that gets passed to `Env` at init, and into the training & eval functions\n",
    "* Agents:\n",
    "  * made 2-hidden-layer by default (was 1-hidden-layer), adjustable number of neurons per hidden layer (`hidden_dim` at init)\n",
    "  * made Tanh on output optional (specify with `output_tanh=True/False` at init)\n",
    "  * added `.predict()` method used at eval time: this one either outputs the mean (`deterministic=True`) or samples an action from the dist (`deterministic=False`). At inference (eval) you want to use deterministic actions, i.e. the mean. During training, we're still using `.forward()` as before, which outputs the mean and std (this is convention). \n",
    "* Added OptimalAgents\n",
    "  * purpose: if you want to train vs a fixed agent\n",
    "  * can be used for training & eval just like a `GaussianPolicy` agent. training functions are adjusted to not try to backprop into these agents (because they don't have any parameters with `require_grad=True`).\n",
    "* Train functions\n",
    "  * defined functions for individual and batched training\n",
    "  * include eval via plotting learned order/price functions at regular intervals  \n",
    "* Eval functions\n",
    "  * added my action (order/price function) plotting function\n",
    "  * made Kristijan's plotting functions consistent: blue dotted is baseline, orange is agent\n",
    "  * cleaned up Kristijan's profit plotting functions: \n",
    "    * now clear about what's being eval'd. E.g. when plotting insider profit, distinguish between simulating `trained I vs trained MM`, `trained I vs optimal MM`, `optimal I vs optimal MM`\n",
    "    * fixed randomness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8137c8",
   "metadata": {},
   "source": [
    "## Imports & Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96217b5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\krist\\AppData\\Local\\Temp\\ipykernel_23216\\4192065543.py:10: UserWarning: A NumPy version >=1.22.4 and <2.3.0 is required for this version of SciPy (detected version 2.3.3)\n",
      "  from scipy.optimize import brentq\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import functools\n",
    "from pettingzoo import AECEnv\n",
    "from gymnasium import spaces\n",
    "from pettingzoo.utils.agent_selector import agent_selector\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import brentq\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6051e6a1",
   "metadata": {},
   "source": [
    "### Eval function definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59dc20e3",
   "metadata": {},
   "source": [
    "Equilibrium parameters and actions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0b931e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def equilibrium_insider_action(v, mean_v, beta):\n",
    "    return beta * (v - mean_v)\n",
    "\n",
    "def equilibrium_market_maker_action(y, mean_v, lambda_):\n",
    "    return mean_v + lambda_ * y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e4a77a",
   "metadata": {},
   "source": [
    "### Env & Agent definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30bf43db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KyleOnePeriodAEC(AECEnv):\n",
    "    metadata = {\n",
    "        \"render_modes\": [\"human\"],\n",
    "        \"name\": \"KyleOnePeriodAEC\",\n",
    "    }\n",
    "\n",
    "    def __init__(self, sigma_v=2.0, sigma_u=1.0, p0=0.5, gamma=0.9, action_scaling=5, T=1):\n",
    "        super().__init__()\n",
    "        self.sigma_v_initial = sigma_v\n",
    "        self.sigma_u_initial = sigma_u\n",
    "        self.p0_initial = p0\n",
    "        self.sigma_v = sigma_v\n",
    "        self.sigma_u = sigma_u\n",
    "        self.p0 = p0\n",
    "        self.timestep = 0\n",
    "        self.gamma = gamma\n",
    "        self.T = T\n",
    "        self.beta = np.zeros(self.T)\n",
    "        self.lmbd = np.zeros(self.T)\n",
    "        self.alpha = np.zeros(self.T)\n",
    "        self.delta = np.zeros(self.T)\n",
    "        self.Sigma = np.zeros(self.T)\n",
    "\n",
    "\n",
    "        self.possible_agents = [\"insider\", \"market_maker\"]\n",
    "        self.agents = self.possible_agents[:]\n",
    "        self.render_mode = None\n",
    "        self.rewards = {a: 0 for a in self.agents}\n",
    "        self.agent_selector = agent_selector(self.agents)\n",
    "\n",
    "        self.action_spaces = {\n",
    "            \"insider\": spaces.Box(low=-1, high=1, shape=(1,), dtype=np.float32),\n",
    "            \"market_maker\": spaces.Box(low=-1, high=1, shape=(1,), dtype=np.float32)\n",
    "        }\n",
    "\n",
    "        self.observation_spaces = {\n",
    "            \"insider\": spaces.Box(low=-10, high=10, shape=(3,), dtype=np.float32),\n",
    "            \"market_maker\": spaces.Box(low=-10, high=10, shape=(3,), dtype=np.float32),\n",
    "        }\n",
    "        self.action_scaling = action_scaling\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        self.agents = self.possible_agents[:]\n",
    "        self.rewards = {a: 0 for a in self.agents}\n",
    "        self._cumulative_rewards = {a: 0 for a in self.agents}\n",
    "        self.terminations = {a: False for a in self.agents}\n",
    "        self.truncations = {a: False for a in self.agents}\n",
    "        self.infos = {a: {} for a in self.agents}\n",
    "        self.timestep = 0\n",
    "        self.sigma_v = self.sigma_v_initial\n",
    "        self.sigma_u = self.sigma_u_initial\n",
    "        self.p0 = self.p0_initial\n",
    "        self.optimal_x = 0\n",
    "        self.optimal_y = 0\n",
    "        self.beta = np.zeros(self.T)\n",
    "        self.lmbd = np.zeros(self.T)\n",
    "        self.alpha = np.zeros(self.T)\n",
    "        self.delta = np.zeros(self.T)\n",
    "        self.Sigma = np.zeros(self.T)\n",
    "\n",
    "        # Sample true value\n",
    "        self.v = torch.normal(self.p0, self.sigma_v, size=(1,)).item()\n",
    "        self.u = torch.normal(0, self.sigma_u, size=(1,)).item()\n",
    "        self.p = 0\n",
    "        self.y = 0\n",
    "\n",
    "        # Set first agent\n",
    "        self.agent_selector.reinit(self.agents)\n",
    "        self.agent_selection = self.agent_selector.next()\n",
    "\n",
    "    def solve_kyle_model(self):\n",
    "        \"\"\"\n",
    "        Calculates the multi-period Kyle model equilibrium using a two-pass algorithm.\n",
    "\n",
    "        Args:\n",
    "            N (int): The total number of trading periods.\n",
    "            sigma2_v (float): The initial variance of the asset's value (Sigma_0).\n",
    "            sigma2_u (float): The variance of the noise trader's orders.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary containing the numpy arrays for each equilibrium\n",
    "                parameter sequence: 'Sigma', 'Lambda', 'Beta', and 'Alpha'.\n",
    "        \"\"\"\n",
    "        # Helper for readability\n",
    "        sigma_u = np.sqrt(self.sigma_u)\n",
    "\n",
    "        # --- ⏪ PASS 1: BACKWARD ITERATION for h_n ---\n",
    "        # h_n = Sigma_n / Sigma_{n-1}\n",
    "        h = np.zeros(self.T)\n",
    "        h[-1] = 0.5  # Terminal condition for the last period (n=N)\n",
    "\n",
    "        # Iterate backwards from n = N-1 down to n=1\n",
    "        for n in reversed(range(self.T - 1)):\n",
    "            # Calculate theta_{n+1} from the known h_{n+1}\n",
    "            h_next = h[n + 1]\n",
    "            theta_next = np.sqrt(h_next / (1 - h_next))\n",
    "\n",
    "            # Define the function whose root h_n we need to find\n",
    "            # f(h_n) = theta_{n+1} - (2*h_n - 1) / (h_n * sqrt(1-h_n)) = 0\n",
    "            def root_func(h_n):\n",
    "                if h_n <= 0.5 or h_n >= 1.0: # Numerical stability\n",
    "                    return np.inf\n",
    "                return theta_next - (2 * h_n - 1) / (h_n * np.sqrt(1 - h_n))\n",
    "\n",
    "            # Find the root h_n in the interval (0.5, 1.0)\n",
    "            # brentq is a robust and efficient choice here.\n",
    "            h[n] = brentq(root_func, 0.5 + 1e-9, 1.0 - 1e-9)\n",
    "\n",
    "        # --- ⏩ PASS 2: FORWARD ITERATION for equilibrium parameters ---\n",
    "        Sigma = np.zeros(self.T + 1)\n",
    "        Lambda = np.zeros(self.T)\n",
    "        Beta = np.zeros(self.T)\n",
    "        Alpha = np.zeros(self.T)\n",
    "\n",
    "        Sigma[0] = self.sigma_v_initial # Initial condition\n",
    "\n",
    "        # Iterate forward from n=1 to N\n",
    "        for n in range(self.T):\n",
    "            # Update variance using the pre-calculated h_n\n",
    "            Sigma[n + 1] = h[n] * Sigma[n]\n",
    "\n",
    "            # Calculate the equilibrium parameters for period n\n",
    "            Lambda[n] = np.sqrt((1 - h[n]) * Sigma[n + 1]) / sigma_u\n",
    "            Beta[n] = (np.sqrt(1 - h[n]) * sigma_u) / np.sqrt(Sigma[n + 1])\n",
    "            Alpha[n] = (sigma_u / (2 * np.sqrt(Sigma[n]))) * np.sqrt(h[n] / (1 - h[n]))\n",
    "        \n",
    "        self.beta = Beta\n",
    "        self.lmbd = Lambda\n",
    "        self.alpha = Alpha\n",
    "        self.h = h\n",
    "        self.Sigma = Sigma\n",
    "\n",
    "        self.delta[self.T-1] = 0  # Terminal condition\n",
    "        \n",
    "        for n in reversed(range(1, self.T-1)):\n",
    "            self.delta[n-1] = self.delta[n] + self.alpha[n] * ( self.lmbd[n] ** 2 ) * self.sigma_u\n",
    "\n",
    "\n",
    "    def observe_insider(self):\n",
    "        return torch.tensor([self.v, self.p0, self.beta[self.timestep]], dtype=torch.float32)\n",
    "\n",
    "    def observe_market_maker(self):\n",
    "        return torch.tensor([self.y, self.p0, self.lmbd[self.timestep]], dtype=torch.float32)\n",
    "\n",
    "    def observe(self, agent):\n",
    "        if agent == \"insider\":\n",
    "            return self.observe_insider()\n",
    "        elif agent == \"market_maker\":\n",
    "            return self.observe_market_maker()\n",
    "\n",
    "    @functools.lru_cache(maxsize=None)\n",
    "    def observation_space(self, agent):\n",
    "        return self.observation_spaces[agent]\n",
    "\n",
    "    @functools.lru_cache(maxsize=None)\n",
    "    def action_space(self, agent):\n",
    "        return self.action_spaces[agent]\n",
    "\n",
    "    def step(self, action):\n",
    "        if self.terminations[self.agent_selection] or self.truncations[self.agent_selection]:\n",
    "            return self._was_dead_step(action)\n",
    "\n",
    "        agent = self.agent_selection\n",
    "\n",
    "        # Store the action\n",
    "        if agent == \"insider\":\n",
    "\n",
    "\n",
    "            self.insider_action = action[0] * self.action_scaling\n",
    "            self.u = torch.normal(0, self.sigma_u, size=(1,)).item()\n",
    "            self.y = self.insider_action + self.u\n",
    "\n",
    "        elif agent == \"market_maker\":\n",
    "            self.market_maker_action = action[0] * self.action_scaling\n",
    "\n",
    "\n",
    "            insider_profit = (self.v - self.market_maker_action) * self.insider_action \n",
    "            market_maker_loss = - (self.market_maker_action - self.v) ** 2\n",
    "\n",
    "            # Assign rewards\n",
    "            self.rewards = {\n",
    "                \"insider\": insider_profit,\n",
    "                \"market_maker\": market_maker_loss\n",
    "            }\n",
    "\n",
    "            self.p0 = equilibrium_market_maker_action(self.y, self.p0, self.lmbd[self.timestep])\n",
    "\n",
    "            # Update timestep\n",
    "            self.timestep += 1\n",
    "            if self.timestep >= self.T:\n",
    "                self.terminations = {a: True for a in self.agents}\n",
    "\n",
    "            # Update cumulative rewards\n",
    "            for a in self.agents:\n",
    "                self._cumulative_rewards[a] += self.rewards[a]\n",
    "\n",
    "        # Get next agent\n",
    "        self.agent_selection = self.agent_selector.next()\n",
    "\n",
    "    def render(self):\n",
    "        print(f\"Step: {self.timestep}\")\n",
    "        print(f\"True value v: {self.v:.2f}\")\n",
    "        print(f\"Order flow y: {self.y:.2f}\")\n",
    "        if hasattr(self, 'rewards'):\n",
    "            print(f\"Agents' rewards: {self._cumulative_rewards}\")\n",
    "\n",
    "    def close(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc4070f",
   "metadata": {},
   "source": [
    "Policy / Actor definition. \n",
    "\n",
    "*Note: this is a 2-hidden-layer arch (default for PPO), can use either ReLU or Tanh activations. For [-1,1] actions people often use Tanh. Experiment with what works better.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14183b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianActor(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim, hidden_dim=64, activation_fn=nn.Tanh, output_tanh=False):\n",
    "        super().__init__()\n",
    "        # self.fc = nn.Sequential(\n",
    "        #     nn.Linear(obs_dim, 64),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Linear(64, act_dim)\n",
    "        # )\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_dim, hidden_dim),\n",
    "            activation_fn(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            activation_fn()\n",
    "        )\n",
    "        self.mean = nn.Linear(hidden_dim, act_dim)\n",
    "        self.log_std = nn.Linear(hidden_dim, act_dim) # State-dependent std\n",
    "        self.output_tanh = output_tanh\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.net(x)\n",
    "        if self.output_tanh:\n",
    "            mean = torch.tanh(self.mean(x))\n",
    "        else:\n",
    "            mean = self.mean(x)\n",
    "        log_std = self.log_std(x)\n",
    "        log_std = torch.clamp(log_std, -8, -1)  # Prevent extreme values    \n",
    "        std = log_std.exp()\n",
    "        return mean, std\n",
    "\n",
    "    def predict(self, x, deterministic=False):\n",
    "        with torch.no_grad():\n",
    "            mean, std = self.forward(x)  # equivalent to self(x)\n",
    "            if deterministic:\n",
    "                action = mean\n",
    "            else:\n",
    "                action = torch.normal(mean, std)\n",
    "            action = action.detach().numpy()\n",
    "            if len(x.shape) == 1:  # single observation, would otherwise be returned as array(action), want just the action\n",
    "                action = action[0]\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700ad4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimalInsider(nn.Module):\n",
    "    def __init__(self, v, p0, beta, action_scaling):\n",
    "        super().__init__()\n",
    "        self.dummy_param = torch.nn.Parameter(torch.zeros(1), requires_grad=False)\n",
    "        self.v = v\n",
    "        self.p0 = p0\n",
    "        self.beta = beta\n",
    "        self.action_scaling = action_scaling  # need b/c any action sampled from this will be scaled by the factor\n",
    "\n",
    "    # def parameters(self):\n",
    "    #     return iter([self.dummy_param])\n",
    "\n",
    "    def forward(self, x):\n",
    "        v = float(x[0])\n",
    "        mean_action = equilibrium_insider_action(v, self.p0, self.beta) / self.action_scaling\n",
    "        mean = torch.tensor([mean_action], requires_grad=False)\n",
    "        std = torch.tensor(1e-10, requires_grad=False).expand_as(mean)\n",
    "        return mean, std\n",
    "    \n",
    "    def predict(self, x, deterministic=True):\n",
    "        mean, std = self.forward(x)\n",
    "        if deterministic:\n",
    "            return mean\n",
    "        else:\n",
    "            return torch.normal(mean, std)\n",
    "    \n",
    "class OptimalMarketMaker(nn.Module):\n",
    "    def __init__(self, y, p0, lmbd, action_scaling):\n",
    "        super().__init__()\n",
    "        self.dummy_param = torch.nn.Parameter(torch.zeros(1), requires_grad=False)\n",
    "        self.y = y\n",
    "        self.p0 = p0\n",
    "        self.lmbd = lmbd\n",
    "        self.action_scaling = action_scaling\n",
    "\n",
    "    # def parameters(self):\n",
    "    #     return iter([self.dummy_param])\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = float(x[0])\n",
    "        mean_action = equilibrium_market_maker_action(y, self.p0, self.lmbd) / self.action_scaling\n",
    "        mean = torch.tensor([mean_action], requires_grad=False)\n",
    "        std = torch.tensor(1e-10, requires_grad=False).expand_as(mean)\n",
    "        return mean, std\n",
    "    \n",
    "    def predict(self, x, deterministic=True):\n",
    "        mean, std = self.forward(x)\n",
    "        if deterministic:\n",
    "            return mean\n",
    "        else:\n",
    "            return torch.normal(mean, std)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a506a4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    def __init__(self, obs_dim, hidden_dim=64, activation_fn=nn.Tanh):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_dim, hidden_dim),\n",
    "            activation_fn(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            activation_fn(),\n",
    "            nn.Linear(hidden_dim, 1)  # Output: V(s)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x).squeeze(-1)  # Return scalar value per observation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28d08f4",
   "metadata": {},
   "source": [
    "### Tests for env & agents\n",
    "\n",
    "Test defining env & agents, and stepping env with deterministic `predict()` actions (what happens during eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d3d170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== stepping with .predict(), simulating eval (deterministic) ===\n",
      "environment step:  0\n",
      "Step: 0\n",
      "True value v: 1.73\n",
      "Order flow y: -1.20\n",
      "Agents' rewards: {'insider': 0, 'market_maker': 0}\n",
      "environment step:  1\n",
      "Step: 1\n",
      "True value v: 1.73\n",
      "Order flow y: -1.20\n",
      "Agents' rewards: {'insider': tensor(-0.3395), 'market_maker': tensor(-3.2545)}\n",
      "environment step:  1\n",
      "Step: 1\n",
      "True value v: 1.73\n",
      "Order flow y: -2.27\n",
      "Agents' rewards: {'insider': tensor(-0.3395), 'market_maker': tensor(-3.2545)}\n",
      "environment step:  2\n",
      "Step: 2\n",
      "True value v: 1.73\n",
      "Order flow y: -2.27\n",
      "Agents' rewards: {'insider': tensor(-0.6842), 'market_maker': tensor(-6.6086)}\n",
      "environment step:  2\n",
      "Step: 2\n",
      "True value v: 1.73\n",
      "Order flow y: -1.22\n",
      "Agents' rewards: {'insider': tensor(-0.6842), 'market_maker': tensor(-6.6086)}\n",
      "environment step:  3\n",
      "Step: 3\n",
      "True value v: 1.73\n",
      "Order flow y: -1.22\n",
      "Agents' rewards: {'insider': tensor(-1.0241), 'market_maker': tensor(-9.8717)}\n",
      "environment step:  3\n",
      "Step: 3\n",
      "True value v: 1.73\n",
      "Order flow y: 0.54\n",
      "Agents' rewards: {'insider': tensor(-1.0241), 'market_maker': tensor(-9.8717)}\n",
      "environment step:  4\n",
      "Step: 4\n",
      "True value v: 1.73\n",
      "Order flow y: 0.54\n",
      "Agents' rewards: {'insider': tensor(-1.3745), 'market_maker': tensor(-13.3385)}\n",
      "environment step:  4\n",
      "Step: 4\n",
      "True value v: 1.73\n",
      "Order flow y: 0.05\n",
      "Agents' rewards: {'insider': tensor(-1.3745), 'market_maker': tensor(-13.3385)}\n",
      "environment step:  5\n",
      "Step: 5\n",
      "True value v: 1.73\n",
      "Order flow y: 0.05\n",
      "Agents' rewards: {'insider': tensor(-1.7245), 'market_maker': tensor(-16.7969)}\n",
      "environment step:  5\n",
      "Step: 5\n",
      "True value v: 1.73\n",
      "Order flow y: 0.66\n",
      "Agents' rewards: {'insider': tensor(-1.7245), 'market_maker': tensor(-16.7969)}\n",
      "environment step:  6\n",
      "Step: 6\n",
      "True value v: 1.73\n",
      "Order flow y: 0.66\n",
      "Agents' rewards: {'insider': tensor(-2.0760), 'market_maker': tensor(-20.2857)}\n",
      "environment step:  6\n",
      "Step: 6\n",
      "True value v: 1.73\n",
      "Order flow y: -0.81\n",
      "Agents' rewards: {'insider': tensor(-2.0760), 'market_maker': tensor(-20.2857)}\n",
      "environment step:  7\n",
      "Step: 7\n",
      "True value v: 1.73\n",
      "Order flow y: -0.81\n",
      "Agents' rewards: {'insider': tensor(-2.4186), 'market_maker': tensor(-23.6007)}\n",
      "environment step:  7\n",
      "Step: 7\n",
      "True value v: 1.73\n",
      "Order flow y: -0.67\n",
      "Agents' rewards: {'insider': tensor(-2.4186), 'market_maker': tensor(-23.6007)}\n",
      "environment step:  8\n",
      "Step: 8\n",
      "True value v: 1.73\n",
      "Order flow y: -0.67\n",
      "Agents' rewards: {'insider': tensor(-2.7640), 'market_maker': tensor(-26.9688)}\n",
      "environment step:  8\n",
      "Step: 8\n",
      "True value v: 1.73\n",
      "Order flow y: 0.39\n",
      "Agents' rewards: {'insider': tensor(-2.7640), 'market_maker': tensor(-26.9688)}\n",
      "environment step:  9\n",
      "Step: 9\n",
      "True value v: 1.73\n",
      "Order flow y: 0.39\n",
      "Agents' rewards: {'insider': tensor(-3.1102), 'market_maker': tensor(-30.3536)}\n",
      "environment step:  9\n",
      "Step: 9\n",
      "True value v: 1.73\n",
      "Order flow y: 1.74\n",
      "Agents' rewards: {'insider': tensor(-3.1102), 'market_maker': tensor(-30.3536)}\n",
      "environment step:  10\n",
      "Step: 10\n",
      "True value v: 1.73\n",
      "Order flow y: 1.74\n",
      "Agents' rewards: {'insider': tensor(-3.4740), 'market_maker': tensor(-34.0904)}\n",
      "environment step:  10\n",
      "Step: 10\n",
      "True value v: 1.73\n",
      "Order flow y: 0.25\n",
      "Agents' rewards: {'insider': tensor(-3.4740), 'market_maker': tensor(-34.0904)}\n",
      "environment step:  11\n",
      "Step: 11\n",
      "True value v: 1.73\n",
      "Order flow y: 0.25\n",
      "Agents' rewards: {'insider': tensor(-3.8223), 'market_maker': tensor(-37.5158)}\n",
      "environment step:  11\n",
      "Step: 11\n",
      "True value v: 1.73\n",
      "Order flow y: 1.15\n",
      "Agents' rewards: {'insider': tensor(-3.8223), 'market_maker': tensor(-37.5158)}\n",
      "environment step:  12\n",
      "Step: 12\n",
      "True value v: 1.73\n",
      "Order flow y: 1.15\n",
      "Agents' rewards: {'insider': tensor(-4.1794), 'market_maker': tensor(-41.1168)}\n",
      "environment step:  12\n",
      "Step: 12\n",
      "True value v: 1.73\n",
      "Order flow y: 0.84\n",
      "Agents' rewards: {'insider': tensor(-4.1794), 'market_maker': tensor(-41.1168)}\n",
      "environment step:  13\n",
      "Step: 13\n",
      "True value v: 1.73\n",
      "Order flow y: 0.84\n",
      "Agents' rewards: {'insider': tensor(-4.5336), 'market_maker': tensor(-44.6595)}\n",
      "environment step:  13\n",
      "Step: 13\n",
      "True value v: 1.73\n",
      "Order flow y: -0.03\n",
      "Agents' rewards: {'insider': tensor(-4.5336), 'market_maker': tensor(-44.6595)}\n",
      "environment step:  14\n",
      "Step: 14\n",
      "True value v: 1.73\n",
      "Order flow y: -0.03\n",
      "Agents' rewards: {'insider': tensor(-4.8843), 'market_maker': tensor(-48.1316)}\n",
      "environment step:  14\n",
      "Step: 14\n",
      "True value v: 1.73\n",
      "Order flow y: -1.27\n",
      "Agents' rewards: {'insider': tensor(-4.8843), 'market_maker': tensor(-48.1316)}\n",
      "environment step:  15\n",
      "Step: 15\n",
      "True value v: 1.73\n",
      "Order flow y: -1.27\n",
      "Agents' rewards: {'insider': tensor(-5.2253), 'market_maker': tensor(-51.4154)}\n",
      "environment step:  15\n",
      "Step: 15\n",
      "True value v: 1.73\n",
      "Order flow y: 0.10\n",
      "Agents' rewards: {'insider': tensor(-5.2253), 'market_maker': tensor(-51.4154)}\n",
      "environment step:  16\n",
      "Step: 16\n",
      "True value v: 1.73\n",
      "Order flow y: 0.10\n",
      "Agents' rewards: {'insider': tensor(-5.5750), 'market_maker': tensor(-54.8681)}\n",
      "environment step:  16\n",
      "Step: 16\n",
      "True value v: 1.73\n",
      "Order flow y: 0.99\n",
      "Agents' rewards: {'insider': tensor(-5.5750), 'market_maker': tensor(-54.8681)}\n",
      "environment step:  17\n",
      "Step: 17\n",
      "True value v: 1.73\n",
      "Order flow y: 0.99\n",
      "Agents' rewards: {'insider': tensor(-5.9312), 'market_maker': tensor(-58.4516)}\n",
      "environment step:  17\n",
      "Step: 17\n",
      "True value v: 1.73\n",
      "Order flow y: -2.14\n",
      "Agents' rewards: {'insider': tensor(-5.9312), 'market_maker': tensor(-58.4516)}\n",
      "environment step:  18\n",
      "Step: 18\n",
      "True value v: 1.73\n",
      "Order flow y: -2.14\n",
      "Agents' rewards: {'insider': tensor(-6.2757), 'market_maker': tensor(-61.8028)}\n",
      "environment step:  18\n",
      "Step: 18\n",
      "True value v: 1.73\n",
      "Order flow y: 0.83\n",
      "Agents' rewards: {'insider': tensor(-6.2757), 'market_maker': tensor(-61.8028)}\n",
      "environment step:  19\n",
      "Step: 19\n",
      "True value v: 1.73\n",
      "Order flow y: 0.83\n",
      "Agents' rewards: {'insider': tensor(-6.6297), 'market_maker': tensor(-65.3405)}\n",
      "environment step:  19\n",
      "Step: 19\n",
      "True value v: 1.73\n",
      "Order flow y: 0.39\n",
      "Agents' rewards: {'insider': tensor(-6.6297), 'market_maker': tensor(-65.3405)}\n",
      "environment step:  20\n",
      "Step: 20\n",
      "True value v: 1.73\n",
      "Order flow y: 0.39\n",
      "Agents' rewards: {'insider': tensor(-6.9759), 'market_maker': tensor(-68.7250)}\n",
      "--all agents done after this one--\n"
     ]
    }
   ],
   "source": [
    "# test env definition etc\n",
    "env = KyleOnePeriodAEC(T=20, sigma_v=1)  # play 3 periods to test multiple steps\n",
    "env.reset()\n",
    "\n",
    "# reset\n",
    "observations = env.reset()\n",
    "done = False\n",
    "\n",
    "# define dummy agents\n",
    "activation_fn = nn.ReLU\n",
    "\n",
    "policies = {\n",
    "    \"insider\": GaussianActor(obs_dim=3, act_dim=1, hidden_dim=64, activation_fn=activation_fn),\n",
    "    \"market_maker\": GaussianActor(obs_dim=3, act_dim=1, hidden_dim=64, activation_fn=activation_fn)\n",
    "}\n",
    "\n",
    "# test stepping env until it terminates (with predict())\n",
    "print(\"=== stepping with .predict(), simulating eval (deterministic) ===\")\n",
    "while not done:  # stops automatically when all agents are done\n",
    "\n",
    "    \n",
    "    agent = env.agent_selection\n",
    "    obs = env.observe(agent)\n",
    "\n",
    "    action = policies[agent].predict(obs, deterministic=True) if not env.terminations[agent] else None\n",
    "    action = torch.tensor([action], dtype=torch.float32)\n",
    "\n",
    "\n",
    "    env.step(action)\n",
    "    print(\"environment step: \", env.timestep)\n",
    "    env.render()\n",
    "\n",
    "        \n",
    "    if env.timestep >= env.T:\n",
    "        done = True\n",
    "        print(\"--all agents done after this one--\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6860a9",
   "metadata": {},
   "source": [
    "Function that evals and plots the learned functions, so:\n",
    "* insider: plot `order(value)`, $I(v) = x$\n",
    "* market maker: plot `price(order_I + order_noise)`, $MM(y) = p$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37fec9ea",
   "metadata": {},
   "source": [
    "dummy optimal insiders and market makers to train against (can drop these in the policies training loop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c28d1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_kyle_model_mine(N: int, sigma2_v: float, sigma2_u: float):\n",
    "    \"\"\"\n",
    "    Calculates the multi-period Kyle model equilibrium using a two-pass algorithm.\n",
    "\n",
    "    Args:\n",
    "        N (int): The total number of trading periods.\n",
    "        sigma2_v (float): The initial variance of the asset's value (Sigma_0).\n",
    "        sigma2_u (float): The variance of the noise trader's orders.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the numpy arrays for each equilibrium\n",
    "              parameter sequence: 'Sigma', 'Lambda', 'Beta', and 'Alpha'.\n",
    "    \"\"\"\n",
    "    # Helper for readability\n",
    "    sigma_u = np.sqrt(sigma2_u)\n",
    "\n",
    "    # --- ⏪ PASS 1: BACKWARD ITERATION for h_n ---\n",
    "    # h_n = Sigma_n / Sigma_{n-1}\n",
    "    h = np.zeros(N)\n",
    "    h[-1] = 0.5  # Terminal condition for the last period (n=N)\n",
    "\n",
    "    # Iterate backwards from n = N-1 down to n=1\n",
    "    for n in reversed(range(N - 1)):\n",
    "        # Calculate theta_{n+1} from the known h_{n+1}\n",
    "        h_next = h[n + 1]\n",
    "        theta_next = np.sqrt(h_next / (1 - h_next))\n",
    "\n",
    "        # Define the function whose root h_n we need to find\n",
    "        # f(h_n) = theta_{n+1} - (2*h_n - 1) / (h_n * sqrt(1-h_n)) = 0\n",
    "        def root_func(h_n):\n",
    "            if h_n <= 0.5 or h_n >= 1.0: # Numerical stability\n",
    "                return np.inf\n",
    "            return theta_next - (2 * h_n - 1) / (h_n * np.sqrt(1 - h_n))\n",
    "\n",
    "        # Find the root h_n in the interval (0.5, 1.0)\n",
    "        # brentq is a robust and efficient choice here.\n",
    "        h[n] = brentq(root_func, 0.5 + 1e-9, 1.0 - 1e-9)\n",
    "\n",
    "    # --- ⏩ PASS 2: FORWARD ITERATION for equilibrium parameters ---\n",
    "    Sigma = np.zeros(N + 1)\n",
    "    Lambda = np.zeros(N)\n",
    "    Beta = np.zeros(N)\n",
    "    Alpha = np.zeros(N)\n",
    "\n",
    "    Sigma[0] = sigma2_v # Initial condition\n",
    "\n",
    "    # Iterate forward from n=1 to N\n",
    "    for n in range(N):\n",
    "        # Update variance using the pre-calculated h_n\n",
    "        Sigma[n + 1] = h[n] * Sigma[n]\n",
    "\n",
    "        # Calculate the equilibrium parameters for period n\n",
    "        Lambda[n] = np.sqrt((1 - h[n]) * Sigma[n + 1]) / sigma_u\n",
    "        Beta[n] = (np.sqrt(1 - h[n]) * sigma_u) / np.sqrt(Sigma[n + 1])\n",
    "        Alpha[n] = (sigma_u / (2 * np.sqrt(Sigma[n]))) * np.sqrt(h[n] / (1 - h[n]))\n",
    "    \n",
    "    return Beta, Lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12274b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9acbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_agent_actions_multiperiod(models, env, n_stdevs, action_scaling=5):\n",
    "    env.reset()\n",
    "    env.solve_kyle_model()\n",
    "    I = models[\"insider\"]\n",
    "    MM = models[\"market_maker\"]\n",
    "\n",
    "    steps = env.T\n",
    "    optimal_insider_actions = np.zeros((steps))\n",
    "    optimal_market_maker_actions = np.zeros((steps))\n",
    "\n",
    "    trained_insider_actions = np.zeros((steps))\n",
    "    trained_market_maker_actions = np.zeros((steps))\n",
    "\n",
    "    values = np.linspace(1, steps, steps)\n",
    "    beta, lmbd = solve_kyle_model_mine(steps, env.sigma_v_initial, env.sigma_u_initial)\n",
    "\n",
    "    p0 = env.p0\n",
    "\n",
    "        \n",
    "\n",
    "    for i in range(len(values)):\n",
    "        \n",
    "        obs_I = env.observe_insider()\n",
    "        trained_insider_actions[i] = I.predict(obs_I, deterministic=True) * action_scaling\n",
    "        optimal_insider_actions[i] = equilibrium_insider_action(env.v, p0, beta[i])\n",
    "        env.step(torch.tensor([trained_insider_actions[i] / action_scaling] , dtype=torch.float32))\n",
    "        u = env.u\n",
    "        y = optimal_insider_actions[i] + u\n",
    "        \n",
    "\n",
    "        obs_MM = env.observe_market_maker()\n",
    "        trained_market_maker_actions[i] = MM.predict(obs_MM, deterministic=True) * action_scaling\n",
    "        optimal_market_maker_actions[i] = equilibrium_market_maker_action(y, p0, lmbd[i])\n",
    "        env.step(torch.tensor([trained_market_maker_actions[i] / action_scaling] , dtype=torch.float32)) \n",
    "        p0 = lmbd[i] * y + p0\n",
    "        loss.append(env.rewards[\"insider\"])\n",
    "        \n",
    "\n",
    "    env.reset()\n",
    "\n",
    "    # plot: 2 subplots. one for insider (values vs action) and one for market maker (order vs action)\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))\n",
    "    \n",
    "    # Insider subplot\n",
    "    ax1.plot(values, optimal_insider_actions, label=\"Optimal\", linestyle='--')\n",
    "    ax1.plot(values, trained_insider_actions, label=\"Trained\", linestyle='-')\n",
    "    ax1.axvline(x=env.p0, color='black', linestyle=':', alpha=0.5)\n",
    "    ax1.axhline(y=0, color='black', linestyle=':', alpha=0.5)\n",
    "    ax1.set_title(\"Insider Action\")\n",
    "    ax1.set_xlabel(\"Timestep t\")\n",
    "    ax1.set_ylabel(\"Action x\")\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Market Maker subplot\n",
    "    ax2.plot(values, optimal_market_maker_actions, label=\"Optimal\", linestyle='--')\n",
    "    ax2.plot(values, trained_market_maker_actions, label=\"Trained\", linestyle='-')\n",
    "    ax2.axvline(x=0, color='black', linestyle=':', alpha=0.5)\n",
    "    ax2.axhline(y=env.p0, color='black', linestyle=':', alpha=0.5)\n",
    "    ax2.set_title(\"Market Maker Action\")\n",
    "    ax2.set_xlabel(\"Timestep t\")\n",
    "    ax2.set_ylabel(\"Price p\")\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd640ec5",
   "metadata": {},
   "source": [
    "### Train function definitions\n",
    "* individual: rollout 1 episode, update agents, repeat for `num_episodes` steps\n",
    "* batched: rollout `batch_size` episodes, update agents (on mean batch rewards, normalized), repeat for `num_iterations` steps\n",
    "  * --> total of `num_iterations * batch_size` episodes rolled out\n",
    "\n",
    "Need to experiment which is better! To me it seems individual is winning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba52a97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_returns(rewards, gamma=1.0):\n",
    "    returns = []\n",
    "    G = 0\n",
    "    if len(rewards) > 1:\n",
    "        # Normalize returns in REINFORCE\n",
    "        returns1 = torch.tensor(rewards)\n",
    "        returns1 = (returns1 - returns1.mean()) / (returns1.std() + 1e-8)\n",
    "    \n",
    "    rewards = returns1.numpy() if len(rewards) > 1 else rewards\n",
    "    rewards = rewards.tolist() if isinstance(rewards, np.ndarray) else rewards\n",
    "\n",
    "    for r in reversed(rewards):\n",
    "        G = r + gamma * G\n",
    "        returns.insert(0, G)\n",
    "\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0dc2ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_debug = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc6a84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_both_reinforce_individual(policies, optimizers, env, num_episodes=10000, plot_every=None, action_scaling=5):\n",
    "    \"\"\"trains both agents using REINFORCE, via `rollout 1 episode, update agents, repeat`\n",
    "    \"\"\"\n",
    "    if plot_every is None:\n",
    "        plot_every = num_episodes // 10\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        # Reset environment\n",
    "        env.reset()\n",
    "        env.solve_kyle_model()\n",
    "\n",
    "        done = False\n",
    "        log_probs = {a: [] for a in env.possible_agents}\n",
    "        rewards = {a: [] for a in env.possible_agents}\n",
    "\n",
    "        # rollout 1 episode\n",
    "        while not done:  # while there are still live agents\n",
    "\n",
    "            agent = env.agent_selection  # whose turn is it?\n",
    "\n",
    "            obs_I = env.observe(agent)\n",
    "\n",
    "            # get action via forward() & sampling\n",
    "            mu, std = policies[agent](obs_I)\n",
    "            \n",
    "            dist = torch.distributions.Normal(mu, std)\n",
    "            action = dist.sample()\n",
    "            log_prob = dist.log_prob(action).sum()\n",
    "\n",
    "            env.step(action.detach().numpy())  # step only this agent!\n",
    "            \n",
    "            # record log_prob for update\n",
    "            log_probs[agent].append(log_prob)\n",
    "\n",
    "            if agent == \"market_maker\":\n",
    "                for agent in env.agents:\n",
    "                    rewards[agent].append(env.rewards[agent])\n",
    "            \n",
    "            if env.timestep >= env.T:\n",
    "                done = True\n",
    "                \n",
    "        # --- Compute returns ---\n",
    "        for agent in env.agents:\n",
    "            rewards[agent] = compute_returns(rewards[agent], env.gamma)\n",
    "            rewards[agent] = torch.tensor(rewards[agent], dtype=torch.float32)\n",
    "\n",
    "        # --- Policy Gradient Update (REINFORCE) ---\n",
    "        for agent in env.possible_agents:\n",
    "            # if agent is optimal, skip training attempt\n",
    "            if not any(p.requires_grad for p in policies[agent].parameters()):\n",
    "                continue\n",
    "            \n",
    "            if len(rewards[agent]) == 0:\n",
    "                continue\n",
    "            \n",
    "            loss = -torch.sum(torch.stack([\n",
    "                log_prob * G_t for log_prob, G_t in zip(log_probs[agent], rewards[agent])\n",
    "            ]))\n",
    "            loss_debug.append(loss.item())\n",
    "            optimizers[agent].zero_grad()\n",
    "            loss.backward()\n",
    "            optimizers[agent].step()\n",
    "\n",
    "        # Eval loop\n",
    "\n",
    "        if (episode+1) % plot_every == 0:\n",
    "            print(f\"Episode: {episode+1}\")\n",
    "            evaluate_agent_actions_multiperiod(policies, env, 2, action_scaling)\n",
    "    return policies, optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9ab142",
   "metadata": {},
   "source": [
    "## Training\n",
    "Set up env, agents, optimizers.\n",
    "\n",
    "**Note:** for action scaling, can either specify scale factor at env definition (default is 5 like before)\n",
    "```\n",
    "env = KyleOnePeriodAEC(action_scaling=action_scale_factor)\n",
    "```\n",
    "\n",
    "or via creating the env without scaling, and wrapping it with supersuit\n",
    "```\n",
    "env = KyleOnePeriodAEC(action_scaling=1)\n",
    "env = scale_actions_v0(env_unscaled, scale=action_scale_factor)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3118c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### PARAMS ####\n",
    "action_scale_factor = 5  # defaults to 5 in env & training function definitions\n",
    "activation_fn = nn.ReLU  # activations for hidden layers. or nn.Tanh (experiment what's better)\n",
    "output_tanh = False  # if True, puts output through nn.Tanh to ensure [-1,1] (experiment)\n",
    "hidden_dim = 32\n",
    "################\n",
    "\n",
    "env = KyleOnePeriodAEC(action_scaling=action_scale_factor, T=25) \n",
    "env.reset()\n",
    "\n",
    "I_obs_dim = env.observation_spaces[\"insider\"].shape[0]\n",
    "I_act_dim = env.action_spaces[\"insider\"].shape[0]\n",
    "MM_obs_dim = env.observation_spaces[\"market_maker\"].shape[0]\n",
    "MM_act_dim = env.action_spaces[\"market_maker\"].shape[0]\n",
    "policies = {\n",
    "    \"insider\": GaussianActor(obs_dim=I_obs_dim, act_dim=I_act_dim, hidden_dim=hidden_dim, activation_fn=activation_fn, output_tanh=output_tanh),\n",
    "    \"market_maker\": GaussianActor(obs_dim=MM_obs_dim, act_dim=MM_act_dim, hidden_dim=hidden_dim, activation_fn=activation_fn, output_tanh=output_tanh)\n",
    "}\n",
    "optimizers = {a: optim.Adam(policies[a].parameters(), lr=1e-4) for a in policies}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db96162",
   "metadata": {},
   "source": [
    "Observe (after 20k episodes of single training)) \n",
    "* activations Tanh, output Tanh: action function too tanh-shaped to converge\n",
    "* activations Tanh, output raw: not quite as much, but still\n",
    "* activations ReLU, output Tanh: decent convergence\n",
    "* activations ReLU, output raw: seems very similar tbh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33256ec7",
   "metadata": {},
   "source": [
    "### Training via \"1 rollout, 1 update, repeat\".\n",
    "* `plot_every` means 'every X iterations, do a round of eval/plotting'. If you leave it as None it'll do it 10 times over the course of the run\n",
    "\n",
    "*Note: unless you reinitialize policies & optimizers, this will continue training existing policies when cell is executed again*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cdf8238",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 500_000\n",
    "plot_every = 1000 # 1_000  # if leave \"None\", will plot 10 times over the course of training\n",
    "policies, optimizers = train_both_reinforce_individual(policies, optimizers, env, num_episodes, plot_every, action_scale_factor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e2f11d",
   "metadata": {},
   "source": [
    "### Training via: \"`batch_size` rollouts, 1 update (on batched means), repeat\".\n",
    "\n",
    "*Note: unless you reinitialize policies & optimizers, this will continue training existing policies when cell is executed again*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1784db88",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 15\n",
    "plot_every = 1 # 1_000  # if leave \"None\", will plot 10 times over the course of training\n",
    "policies, optimizers = train_both_reinforce_individual(policies, optimizers, env, num_episodes, plot_every, action_scale_factor)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
